{"id": "c5357f3e-ac84-4b0c-99b3-89f7a46e798c", "displayed_text": "Title: On the Sample Complexity of Stabilizing LTI Systems on a Single Trajectory\nAbstract: Stabilizing an unknown dynamical system is one of the central problems in control theory. In this paper, we study the sample complexity of the learn-to-stabilize problem in Linear Time- Invariant (LTI) systems on a single trajectory. Current state-of-the-art approaches require a sample complexity linear in n, the state dimension, which incurs a state norm that blows up exponentially in n. We propose a novel algorithm based on spectral decomposition that only needs to learn \u00d2a small part\u00d3 of the dynamical matrix acting on its unstable subspace. We show that, under proper assumptions, our algorithm stabilizes an LTI system on a single trajectory with O__(k) samples, where k is the instability index of the system. This represents the first sub-linear sample complex- ity result for the stabilization of LTI systems under the regime when k = o(n).", "label_annotations": {"Multi-aspect Summary": {"Context": "Stabilizing unknown LTI systems with current methods requires a sample complexity linear in the state dimension, causing exponential state norm growth.", "Key idea": "The paper proposes an algorithm using spectral decomposition to learn only the unstable subspace of the dynamical matrix, reducing sample complexity.", "Validation": "The study theoretically demonstrates that the algorithm stabilizes an LTI system on a single trajectory with O(k) samples, where k is the instability index.\r\n", "Outcome": "The algorithm achieves the first sub-linear sample complexity for stabilizing LTI systems when k = o(n)", "Future Impact": " This approach could significantly improve control theory methods, reducing sample complexity and inspiring further research in efficient system stabilization.\r\n"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 34s "}}
{"id": "660fb729-de11-4918-a54f-a02b80fdbd69", "displayed_text": "Title: OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression\nAbstract: This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings; The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing ordinal regression methods often overfit and underperform, deriving rank concepts mainly from training sets.", "Key idea": "The paper proposes OrdinalCLIP, a method using CLIP's semantic latent space and differentiable prompting for ordinal regression.", "Validation": " The study uses experimental setups to test OrdinalCLIP's performance in general ordinal regression tasks, few-shot, and distribution shift settings.", "Outcome": "OrdinalCLIP achieves competitive performance in general ordinal regression and shows improvements in few-shot and distribution shift settings for age estimation.", "Future Impact": "The approach suggests a new direction for efficient ordinal regression, reducing computational overhead and encouraging further research on language-guided models."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 26s "}}
