{"id": "4c81f948-f4b2-4130-92e5-623d942f0859", "displayed_text": "Title: DropCov: A Simple yet Effective Method for Improving Deep Architectures\nAbstract: Previous works show global covariance pooling (GCP) has great potential to improve deep architectures especially on visual recognition tasks, where post- normalization of GCP plays a very important role in final performance.  Although several post-normalization strategies have been studied, these methods pay more close attention to effect of normalization on covariance representations rather than the whole GCP networks, and their effectiveness requires further understanding.  Meanwhile, existing effective post-normalization strategies (e.g., matrix power normalization) usually suffer from high computational complexity (e.g., O(d3) for d-dimensional inputs).  To handle above issues, this work first analyzes the effect of post-normalization from the perspective of training GCP networks.  Particularly, we for the first time show that effective post-normalization can make a good trade-off between representation decorrelation and information preservation for GCP, which are crucial to alleviate over-fitting and increase representation ability of deep GCP networks, respectively.  Based on this finding, we can improve existing post- normalization methods with some small modifications, providing further support to our observation.  Furthermore, this finding encourages us to propose a novel pre-normalization method for GCP (namely DropCov), which develops an adaptive channel dropout on features right before GCP, aiming to reach trade-off between representation decorrelation and information preservation in a more efficient way.  Our DropCov only has a linear complexity of O(d), while being free for inference.  Extensive experiments on various benchmarks (i.e., ImageNet-1K, ImageNet-C, ImageNet-A, Stylized-ImageNet, and iNat2017) show our DropCov is superior to the counterparts in terms of efficiency and effectiveness, and provides a simple yet effective method to improve performance of deep architectures involving both deep convolutional neural networks (CNNs) and vision transformers (ViTs).", "label_annotations": {"Multi-aspect Summary": {"Context": "Prior works highlight the potential of global covariance pooling (GCP) in enhancing deep architectures, especially in visual recognition tasks, but existing post-normalization methods lack a holistic approach to optimizing GCP networks.", "Key idea": "The paper introduces DropCov, a pre-normalization method for GCP, leveraging adaptive channel dropout to balance representation decorrelation and information preservation efficiently.", "Validation": " Extensive experiments on various benchmarks demonstrate that DropCov outperforms existing methods in terms of efficiency and effectiveness, validating its superiority.", "Outcome": "DropCov significantly improves the performance of deep architectures, particularly CNNs and ViTs, showcasing its simplicity and effectiveness in enhancing deep learning models.", "Future Impact": "The proposed method could lead to advancements in deep architecture optimization, influencing future research on efficient representation learning in deep learning models."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 4s "}}
{"id": "c5357f3e-ac84-4b0c-99b3-89f7a46e798c", "displayed_text": "Title: On the Sample Complexity of Stabilizing LTI Systems on a Single Trajectory\nAbstract: Stabilizing an unknown dynamical system is one of the central problems in control theory. In this paper, we study the sample complexity of the learn-to-stabilize problem in Linear Time- Invariant (LTI) systems on a single trajectory. Current state-of-the-art approaches require a sample complexity linear in n, the state dimension, which incurs a state norm that blows up exponentially in n. We propose a novel algorithm based on spectral decomposition that only needs to learn \u00d2a small part\u00d3 of the dynamical matrix acting on its unstable subspace. We show that, under proper assumptions, our algorithm stabilizes an LTI system on a single trajectory with O__(k) samples, where k is the instability index of the system. This represents the first sub-linear sample complex- ity result for the stabilization of LTI systems under the regime when k = o(n).", "label_annotations": {"Multi-aspect Summary": {"Context": "Current approaches to stabilizing LTI systems have a linear sample complexity in the state dimension, leading to exponential growth in state norm", "Key idea": "The paper proposes a novel algorithm based on spectral decomposition, requiring learning of a small part of the dynamical matrix in the unstable subspace, achieving sub-linear sample complexity O(k) where k is the instability index.", "Validation": "The algorithm is shown to stabilize LTI systems on a single trajectory under certain assumptions, demonstrating its effectiveness in reducing sample complexity.", "Outcome": "This work represents the first sub-linear sample complexity result for stabilizing LTI systems, offering a significant improvement over existing methods.", "Future Impact": "The algorithm's efficiency may lead to advancements in stabilizing unknown dynamical systems, potentially influencing future research in control theory and related fields."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 55s "}}
