{"user_id": "orange@umich.edu", "instance_id": "4c81f948-f4b2-4130-92e5-623d942f0859", "displayed_text": "Title: DropCov: A Simple yet Effective Method for Improving Deep Architectures\nAbstract: Previous works show global covariance pooling (GCP) has great potential to improve deep architectures especially on visual recognition tasks, where post- normalization of GCP plays a very important role in final performance.  Although several post-normalization strategies have been studied, these methods pay more close attention to effect of normalization on covariance representations rather than the whole GCP networks, and their effectiveness requires further understanding.  Meanwhile, existing effective post-normalization strategies (e.g., matrix power normalization) usually suffer from high computational complexity (e.g., O(d3) for d-dimensional inputs).  To handle above issues, this work first analyzes the effect of post-normalization from the perspective of training GCP networks.  Particularly, we for the first time show that effective post-normalization can make a good trade-off between representation decorrelation and information preservation for GCP, which are crucial to alleviate over-fitting and increase representation ability of deep GCP networks, respectively.  Based on this finding, we can improve existing post- normalization methods with some small modifications, providing further support to our observation.  Furthermore, this finding encourages us to propose a novel pre-normalization method for GCP (namely DropCov), which develops an adaptive channel dropout on features right before GCP, aiming to reach trade-off between representation decorrelation and information preservation in a more efficient way.  Our DropCov only has a linear complexity of O(d), while being free for inference.  Extensive experiments on various benchmarks (i.e., ImageNet-1K, ImageNet-C, ImageNet-A, Stylized-ImageNet, and iNat2017) show our DropCov is superior to the counterparts in terms of efficiency and effectiveness, and provides a simple yet effective method to improve performance of deep architectures involving both deep convolutional neural networks (CNNs) and vision transformers (ViTs).", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing global covariance pooling (GCP) methods improve deep architectures but suffer from high computational complexity and suboptimal post-normalization effectiveness.", "Key idea": " The paper proposes DropCov, a pre-normalization method using adaptive channel dropout to balance representation decorrelation and information preservation efficiently.", "Validation": "The study uses extensive experiments on benchmarks like ImageNet-1K, ImageNet-C, ImageNet-A, Stylized-ImageNet, and iNat2017 to validate DropCov's performance.", "Outcome": "DropCov outperforms existing methods in efficiency and effectiveness, providing a linear complexity solution for improving deep architectures.", "Future Impact": "This method could lead to more efficient deep learning models, inspiring further research on pre-normalization techniques and their applications."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 14s "}}
{"user_id": "orange@umich.edu", "instance_id": "e6e3b00f-54a1-4d4b-a927-c669559491fd", "displayed_text": "Title: Geometry-aware Instance-reweighted Adversarial Training\nAbstract: In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direc- tion, we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing ef- fect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding ad- versarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversar- ial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "label_annotations": {"Multi-aspect Summary": {"Context": "Traditional belief holds that robustness and accuracy in adversarial machine learning are mutually exclusive, motivating the exploration of improving robustness without sacrificing accuracy.", "Key idea": "The paper introduces geometry-aware instance-reweighted adversarial training, assigning weights based on the difficulty of attacking natural data points.", "Validation": "Experiments are conducted to show that the proposed method enhances the robustness of standard adversarial training while maintaining accuracy.", "Outcome": "The proposed method successfully improves both robustness and accuracy in adversarial training.", "Future Impact": "This approach may inspire new strategies in adversarial training, balancing robustness and accuracy, and encouraging further research in this direction.\r\n"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 47s "}}
{"user_id": "xziyang@umich.edu", "instance_id": "3477752b-3858-4954-80f0-565df69fbb55", "displayed_text": "Title: Sqrt(d) Dimension Dependence of Langevin Monte Carlo\nAbstract: This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al. (2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an O(sqrt(d)/sigma) mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known O(d/sigma)  result and is optimal (in terms of order) in both dimension d and accuracy tolerance _ for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments.", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous research has not sufficiently addressed the sampling error of unadjusted Langevin Monte Carlo (LMC) in 2-Wasserstein distance under certain conditions.", "Key idea": "This paper refines the mean-square analysis to provide a non-asymptotic analysis of LMC's sampling error, establishing a new mixing time bound.", "Validation": "The study uses a theoretical framework based on discretizations of contractive SDEs and analyzes sampling error under log-smooth, log-strongly-convex conditions.", "Outcome": "The new framework improves the best-known mixing time bound for LMC, providing more accurate predictions without the need for a warm start.", "Future Impact": "The refined analysis method may inspire further research into more efficient MCMC algorithms and broader applications in sampling and optimization problems."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 18s "}}
{"user_id": "xziyang@umich.edu", "instance_id": "e6e3b00f-54a1-4d4b-a927-c669559491fd", "displayed_text": "Title: Geometry-aware Instance-reweighted Adversarial Training\nAbstract: In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direc- tion, we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing ef- fect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding ad- versarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversar- ial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.", "label_annotations": {"Multi-aspect Summary": {"Context": "The belief that robustness and accuracy in adversarial machine learning are mutually exclusive motivates this study.", "Key idea": "The paper proposes geometry-aware instance-reweighted adversarial training, assigning weights to adversarial data based on attack difficulty.", "Validation": "The study uses experimental setups to demonstrate the effectiveness of geometry-aware instance-reweighted adversarial training.", "Outcome": "The proposed method improves both robustness and accuracy in standard adversarial training.", "Future Impact": "The approach suggests new avenues for enhancing model robustness without sacrificing accuracy, encouraging further research in adversarial training techniques."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 8s "}}
{"user_id": "apple@umich.edu", "instance_id": "660fb729-de11-4918-a54f-a02b80fdbd69", "displayed_text": "Title: OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression\nAbstract: This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings; The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing ordinal regression methods often overfit and underperform, deriving rank concepts mainly from training sets.", "Key idea": "The paper proposes OrdinalCLIP, a method using CLIP's semantic latent space and differentiable prompting for ordinal regression.", "Validation": " The study uses experimental setups to test OrdinalCLIP's performance in general ordinal regression tasks, few-shot, and distribution shift settings.", "Outcome": "OrdinalCLIP achieves competitive performance in general ordinal regression and shows improvements in few-shot and distribution shift settings for age estimation.", "Future Impact": "The approach suggests a new direction for efficient ordinal regression, reducing computational overhead and encouraging further research on language-guided models."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 26s "}}
{"user_id": "apple@umich.edu", "instance_id": "c5357f3e-ac84-4b0c-99b3-89f7a46e798c", "displayed_text": "Title: On the Sample Complexity of Stabilizing LTI Systems on a Single Trajectory\nAbstract: Stabilizing an unknown dynamical system is one of the central problems in control theory. In this paper, we study the sample complexity of the learn-to-stabilize problem in Linear Time- Invariant (LTI) systems on a single trajectory. Current state-of-the-art approaches require a sample complexity linear in n, the state dimension, which incurs a state norm that blows up exponentially in n. We propose a novel algorithm based on spectral decomposition that only needs to learn \u00d2a small part\u00d3 of the dynamical matrix acting on its unstable subspace. We show that, under proper assumptions, our algorithm stabilizes an LTI system on a single trajectory with O__(k) samples, where k is the instability index of the system. This represents the first sub-linear sample complex- ity result for the stabilization of LTI systems under the regime when k = o(n).", "label_annotations": {"Multi-aspect Summary": {"Context": "Stabilizing unknown LTI systems with current methods requires a sample complexity linear in the state dimension, causing exponential state norm growth.", "Key idea": "The paper proposes an algorithm using spectral decomposition to learn only the unstable subspace of the dynamical matrix, reducing sample complexity.", "Validation": "The study theoretically demonstrates that the algorithm stabilizes an LTI system on a single trajectory with O(k) samples, where k is the instability index.\r\n", "Outcome": "The algorithm achieves the first sub-linear sample complexity for stabilizing LTI systems when k = o(n)", "Future Impact": " This approach could significantly improve control theory methods, reducing sample complexity and inspiring further research in efficient system stabilization.\r\n"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 34s "}}
{"user_id": "banana@umich.edu", "instance_id": "660fb729-de11-4918-a54f-a02b80fdbd69", "displayed_text": "Title: OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression\nAbstract: This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings; The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing ordinal regression methods often overfit and underperform by treating each rank as a separate category, deriving concepts mainly from training data.", "Key idea": "The paper proposes OrdinalCLIP, using CLIP's semantic latent space and differentiable prompting to learn rank concepts as language prototypes.\r\n", "Validation": "The study uses experimental results from general ordinal regression tasks, few-shot, and distribution shift settings for age estimation to validate the method.", "Outcome": " OrdinalCLIP achieves competitive performance in general ordinal regression tasks and improves results in few-shot and distribution shift settings.", "Future Impact": " The approach suggests efficient ordinal regression techniques, reducing computational overhead and encouraging further research on language-guided models."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"user_id": "banana@umich.edu", "instance_id": "3477752b-3858-4954-80f0-565df69fbb55", "displayed_text": "Title: Sqrt(d) Dimension Dependence of Langevin Monte Carlo\nAbstract: This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al. (2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an O(sqrt(d)/sigma) mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known O(d/sigma)  result and is optimal (in terms of order) in both dimension d and accuracy tolerance _ for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments.", "label_annotations": {"Multi-aspect Summary": {"Context": "This study addresses the sampling error analysis of Langevin Monte Carlo (LMC) in 2-Wasserstein distance, enhancing understanding of MCMC methods for contractive SDEs discretizations.", "Key idea": "The paper refines mean-square analysis to automate the analysis of sampling algorithms, establishing an optimal  O( d/\u03f5) mixing time bound for LMC under specific conditions, improving previous results.\r\n", "Validation": "The study's theoretical analysis is validated through numerical experiments, confirming the effectiveness of the proposed approach", "Outcome": "The paper achieves an optimal mixing time bound for LMC, improving understanding of MCMC methods' efficiency and accuracy in high-dimensional spaces.", "Future Impact": "This work may lead to advancements in MCMC methods, particularly in improving sampling efficiency and accuracy in high-dimensional spaces, encouraging further research in this area."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 30s "}}
{"user_id": "pear@umich.edu", "instance_id": "4c81f948-f4b2-4130-92e5-623d942f0859", "displayed_text": "Title: DropCov: A Simple yet Effective Method for Improving Deep Architectures\nAbstract: Previous works show global covariance pooling (GCP) has great potential to improve deep architectures especially on visual recognition tasks, where post- normalization of GCP plays a very important role in final performance.  Although several post-normalization strategies have been studied, these methods pay more close attention to effect of normalization on covariance representations rather than the whole GCP networks, and their effectiveness requires further understanding.  Meanwhile, existing effective post-normalization strategies (e.g., matrix power normalization) usually suffer from high computational complexity (e.g., O(d3) for d-dimensional inputs).  To handle above issues, this work first analyzes the effect of post-normalization from the perspective of training GCP networks.  Particularly, we for the first time show that effective post-normalization can make a good trade-off between representation decorrelation and information preservation for GCP, which are crucial to alleviate over-fitting and increase representation ability of deep GCP networks, respectively.  Based on this finding, we can improve existing post- normalization methods with some small modifications, providing further support to our observation.  Furthermore, this finding encourages us to propose a novel pre-normalization method for GCP (namely DropCov), which develops an adaptive channel dropout on features right before GCP, aiming to reach trade-off between representation decorrelation and information preservation in a more efficient way.  Our DropCov only has a linear complexity of O(d), while being free for inference.  Extensive experiments on various benchmarks (i.e., ImageNet-1K, ImageNet-C, ImageNet-A, Stylized-ImageNet, and iNat2017) show our DropCov is superior to the counterparts in terms of efficiency and effectiveness, and provides a simple yet effective method to improve performance of deep architectures involving both deep convolutional neural networks (CNNs) and vision transformers (ViTs).", "label_annotations": {"Multi-aspect Summary": {"Context": "Prior works highlight the potential of global covariance pooling (GCP) in enhancing deep architectures, especially in visual recognition tasks, but existing post-normalization methods lack a holistic approach to optimizing GCP networks.", "Key idea": "The paper introduces DropCov, a pre-normalization method for GCP, leveraging adaptive channel dropout to balance representation decorrelation and information preservation efficiently.", "Validation": " Extensive experiments on various benchmarks demonstrate that DropCov outperforms existing methods in terms of efficiency and effectiveness, validating its superiority.", "Outcome": "DropCov significantly improves the performance of deep architectures, particularly CNNs and ViTs, showcasing its simplicity and effectiveness in enhancing deep learning models.", "Future Impact": "The proposed method could lead to advancements in deep architecture optimization, influencing future research on efficient representation learning in deep learning models."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 4s "}}
{"user_id": "pear@umich.edu", "instance_id": "c5357f3e-ac84-4b0c-99b3-89f7a46e798c", "displayed_text": "Title: On the Sample Complexity of Stabilizing LTI Systems on a Single Trajectory\nAbstract: Stabilizing an unknown dynamical system is one of the central problems in control theory. In this paper, we study the sample complexity of the learn-to-stabilize problem in Linear Time- Invariant (LTI) systems on a single trajectory. Current state-of-the-art approaches require a sample complexity linear in n, the state dimension, which incurs a state norm that blows up exponentially in n. We propose a novel algorithm based on spectral decomposition that only needs to learn \u00d2a small part\u00d3 of the dynamical matrix acting on its unstable subspace. We show that, under proper assumptions, our algorithm stabilizes an LTI system on a single trajectory with O__(k) samples, where k is the instability index of the system. This represents the first sub-linear sample complex- ity result for the stabilization of LTI systems under the regime when k = o(n).", "label_annotations": {"Multi-aspect Summary": {"Context": "Current approaches to stabilizing LTI systems have a linear sample complexity in the state dimension, leading to exponential growth in state norm", "Key idea": "The paper proposes a novel algorithm based on spectral decomposition, requiring learning of a small part of the dynamical matrix in the unstable subspace, achieving sub-linear sample complexity O(k) where k is the instability index.", "Validation": "The algorithm is shown to stabilize LTI systems on a single trajectory under certain assumptions, demonstrating its effectiveness in reducing sample complexity.", "Outcome": "This work represents the first sub-linear sample complexity result for stabilizing LTI systems, offering a significant improvement over existing methods.", "Future Impact": "The algorithm's efficiency may lead to advancements in stabilizing unknown dynamical systems, potentially influencing future research in control theory and related fields."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 55s "}}
